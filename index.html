<!DOCTYPE html>
<html lang="en">

<head>
    <meta name="generator" content="HTML Tidy for HTML5 for Apple macOS version 5.6.0">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="AI and Security Workshop (AISec)">
    <meta name="keywords"
        content="Deep Learning, Machine Learning, Security, Adversarial Examples, Attacks, Intrusion Detection, Program Analysis, Malware, Botnets, Vulnerability, Phishing, Forensics, Neural Networks, Recurrent Networks, Generative Adversarial Networks, AISec">
    <meta name="author" content="AISec Chairs">
    <title>16th ACM Workshop on Artificial Intelligence and Security (AISec 2023)</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha3/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-KK94CHFLLe+nY2dmCWGMq91rCGa5gtU4mk92HdvYe+M/SXH301p5ILy+dN9+nJOZ" crossorigin="anonymous">

    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=K2D:400,700' rel='stylesheet' type='text/css'>
    <!-- Custom styles for this template -->
    <link href="css/agency.css" rel="stylesheet">

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-QDQDHN7F62"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-QDQDHN7F62');
    </script>

</head>

<body id="page-top">
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-dark fixed-top" id="mainNav">
        <div class="container">
            <a class="navbar-brand js-scroll-trigger" href="#page-top">AISec 2023</a> <button
                class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse"
                data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false"
                aria-label="Toggle navigation">Menu</button>
            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav text-uppercase ml-auto">
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="#page-top">Home</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="#keynote">Keynotes</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="#programme">Programme</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="#accepted">Accepted Papers</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="#cfp">Call for Papers</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="#award">Best Paper Award</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="#committee">Committee</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" target="_blank"
                            href="https://www.sigsac.org/ccs/CCS2023/">ACM CCS</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Header -->
    <header class="masthead">
        <div class="container">
            <div class="intro-text">
                <div class="intro-heading">
                    16<sup><small><b>th</b></small></sup> ACM Workshop on <br>
                    Artificial Intelligence and Security
                </div>
                <div class="intro-lead-in">
                    <b>November 30, 2023</b> — Copenhagen</b>
                </div>
                <div class="intro-lead-in">
                    co-located with the 30th ACM Conference on Computer and Communications Security
                </div>
                <div class="photo-credit">
                    Photo: <a target="_blank" href="https://pixabay.com/">Pixabay</a>
                </div>
            </div>
        </div>
    </header>


    <!-- Keynotes -->

    <section id="keynote">
        <div class="container">
            <div class="row">
            <h2 class="section-heading text-uppercase">Keynotes</h2>
            </div>

            <div class="row">
                <div class="col-lg-3">
                    <center><img src="img/troncoso.jpg" class="portait"></center>
                </div>
                <div class="col-lg-9 text-justify">
                    <h3 class="section-subheading"><b>Title: When decentralization, security, and privacy are not friends</b></h3>
                        <details><summary><b>Carmela Troncoso, Associate Professor @ EPFL</b></summary>
                            <p>Carmela Troncoso is an Associate Professor at EPFL (Switzerland) where she heads the SPRING Lab. Her work focuses on analyzing, building, and deploying secure and privacy-preserving systems. Troncoso holds a Ph.D. in engineering from KULeuven. Her work on privacy engineering has received multiple awards, and she has been named 40 under 40 in technology by Fortune in 2020.</p>
                        </details>                        
                        <p>Decentralization is often seen as a main tool to achieve security and privacy. It has worked in a number of systems, for which decentralization help protect identities and data of users. Thus, it is not a surprise that a new trend of machine learning algorithms opt for decentralization to increase data privacy. In this talk, we will analyze decentralized machine learning proposals and show how they not only don’t improve privacy or robustness, but also increase the surface of attack resulting in less protection than federated alternatives.</p>
                </div>
            </div>

           
            
            <div class="row">
                <div class="col-lg-3">
                    <center><img src="img/rodriguez.jpeg" class="portait"></center>
                </div>
                <div class="col-lg-9 text-justify">
                    <h3 class="section-subheading"><b>Title: Emerging challenges in securing frontier AI systems</b></h3>
                    <details>
                    <summary><b>Mikel Rodriguez, AI Red Teaming @ Google Deepmind</b></summary>
                        <p>
                            Dr. Mikel Rodriguez has spent over two decades working in the public and private sector securing the application of Artificial Intelligence in high-stakes consequential environments. At Google DeepMind, Mikel defines and leads the cross-functional AI Red and Blue “ReBl” team to ensure that foundational models are battle-tested with the rigor and scrutiny of real-world adversaries, and help drive research and tooling that will make this red-blue mindset scalable in preparation for AGI.
                            In his role as the Managing Director at MITRE Labs, Mikel built and led the AI Red Team that focuses on deployed AI systems that can be susceptible to bias in their data, attacks involving evasion, data poisoning, model replication; and the exploitation of software flaws to deceive, manipulate, compromise, and render them ineffective. Mikel’s team worked on developing methods to mitigate bias and defend against emerging ML attacks, securing the AI supply chain, and generally ensuring the trustworthiness of AI systems so they perform as intended in mission-critical environments. While at MITRE, his team in collaboration with many industry partners, published ATLAS (Adversarial Threat Landscape for AI Systems) - a knowledge base of adversary tactics, techniques, and case studies for machine learning (ML) systems based on real-world observations, demonstrations from ML red teams and security groups, and the state of the possible from academic research.  Mikel firmly believes that AI’s potential will only be realized through collaborations that help produce reliable, resilient, fair, interpretable, privacy preserving, and secure technologies.
                            Mikel received his Ph.D. in 2010 while working at University of Central Florida’s computer vision lab with professor Mubarak Shah. He then moved to Paris where he worked as a post-doctoral research fellow at INRIA.
                        </p>
                    </details>
                    <p>As advanced AI assistants have become more general purpose, sophisticated and capable, they create new opportunities in a variety of fields, such as education, science and healthcare. Yet the rapid speed of progress has made it difficult to adequately prepare for, or even understand, security and privacy vulnerabilities that may emerge as a result of these new capabilities.  
                        Several foreseeable developments in advanced AI assistants including tool use, multimodality, planning and deeper reasoning, and memory have the potential to significantly expand the security and misuse risk profile of these systems. In this talk we will explore a number of best practices and future research directions that can help us better prepare society for managing these risks.
                        </p>
                    <p>
                    </p>
                    
                </div>
            </div>

            <div class="row">
                <div class="col-lg-3">
                    <center><img src="img/fritz.jpg" class="portait"></center>
                </div>
                <div class="col-lg-9 text-justify">
                    <h3 class="section-subheading"><b>Title: Trustworthy AI and A Cybersecurity Perspective on Large Language Models</b></h3>
                        <details>
                            <summary><b>Mario Fritz, Faculty @ CISPA Helmholtz Center for Information Security</b></summary>
                            <p>
                                Prof. Dr. Mario Fritz is a faculty at the CISPA Helmholtz Center for Information Security, an honorary professor at Saarland University, and a fellow of the European Laboratory for Learning and Intelligent Systems (ELLIS). Until 2018, he led a research group at the Max Planck Institute for Computer Science. Previously, he was a PostDoc at the International Computer Science Institute (ICSI) and UC Berkeley after receiving his PhD from TU Darmstadt and studying computer science at FAU Erlangen-Nuremberg. His research focuses on trustworthy artificial intelligence, especially at the intersection of information security and machine learning. He is Associate Editor of the journal "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) and has published over 100 articles in top conferences and journals. Currently, he is coordinating the Network of Excellence in AI "ELSA -- European Lighthouse on Secure and Safe AI" which is an ELLIS (https://ellis.eu/) initiative that is funded by the EU and connects universities, research institutes, and industry partners across Europe (elsa-ai.eu).
                            </p>
                    </details>
                    <p>
                        As AI technology is getting increasingly mature, we see a broad deployment of AI in many application domains. However, this increases the demands on properties related to trustworthiness like robustness, privacy, transparency, accountability as well as explainability. Besides the trustworthiness of AI, misinformation and deepfakes are becoming key concerns in terms of the negative effects that AI can have on society. I'll discuss the larger ecosystem around misinformation and different approaches to mitigate these pressing issues in the future. Finally, Large Language Models (LLMs) like GPT4 have demonstrated how AI deployment is reaching millions of users, which in turn puts a magnifying glass on some of the issues mentioned before. I'll demonstrate cybersecurity concerns and threats that emerge from the recent trend of application-integrated LLMs and AI assistants as well as sketch how future development will initiate new research challenges in this domain.
                    </p>
                    <p>
                    </p>
                    
                </div>
            </div>



            </div>
        </div>
    </section>


    <!-- Programme -->
    <section class="bg-light" id="programme">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-justify">
                    <h2 class="section-heading text-uppercase">Programme</h2>

                    <table cellpadding="5">

                        <p>The following times are on CET (UTC +1).</p>
                        <tr>
                            <td class="orga" width="120px">09:00&ndash;9:15</td>
                            <td class="orga">Opening and Welcome</td>
                        </tr>

                        <tr>
                            <td class="orga" width="120px">9:15&ndash;10:00</td>
                            <td class="orga uline">Keynote 1</td>
                            </tr>
                            <tr>
                                <td></td>
                                <td>
                                <em class="paper">When decentralization, security, and privacy are not friends</em>
                                <br><b>Carmela Troncoso</b>, Associate Professor @ EPFL
                            </td>

                        </tr>

                        <tr>
                            <td class="orga">10:00&ndash;10:20</td>
                            <td class="orga">Coffee break</td>
                        </tr>

                        <tr>
                            <td class="orga">10:20-11:00</td>
                            <td class="orga uline">Spotlights</td>
                            <tr>
                                <td></td>
                                <td><em class="paper">When Side-Channel Attacks Break the Black-Box Property of Embedded Artificial Intelligence</em>
                                    <br><b>Authors</b>: Benoit Coqueret (Univ. Rennes, Inria), Mathieu Carbone (Thales ITSEF), Olivier Sentieys (Univ. Rennes, Inria), Gabriel Zaid (Thales ITSEF)
                                </td>
                            </tr>
                            <tr>
                                <td></td>
                                <td><em class="paper">Lookin' Out My Backdoor! Investigating Backdooring Attacks Against DL-driven Malware Detectors</em>
                                    <br><b>Authors</b>: Mario D'Onghia (Politecnico di Milano), Federico Di Cesare (Politecnico di Milano), Luigi Gallo (Cyber Security Lab, Telecom Italia), Michele Carminati (Politecnico di Milano), Mario Polino (Politecnico di Milano), Stefano Zanero (Politecnico di Milano)
                                </td>
                            </tr>
                            <tr>
                                <td></td>
                                <td><em class="paper">Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection</em>
                                    <br><b>Authors</b>: Sahar Abdelnabi (CISPA Helmholtz Center for Information Security), Kai Greshake (Saarland University, sequire technology GmbH), Shailesh Mishra (Saarland University), Christoph Endres (sequire technology GmbH), Thorsten Holz (CISPA Helmholtz Center for Information Security), Mario Fritz (CISPA Helmholtz Center for Information Security)
                                </td>
                            </tr>
                            <tr>
                                <td></td>
                                <td><em class="paper">Canaries and Whistles: Resilient Drone Communication Networks with (or without) Deep Reinforcement Learning</em>
                                    <br><b>Authors</b>: Chris Hicks (The Alan Turing Institute), Vasilios Mavroudis (The Alan Turing Institute), Myles Foley (Imperial College London), Thomas Davies (The Alan Turing Institute), Kate Highnam (Imperial College London), Tim Watson (The Alan Turing Institute)
                                </td>
                            </tr>
                        </tr>

                        <tr>
                            <td class="orga">11:00&ndash;12:00</td>
                            <td class="orga uline">Poster session 1</td>
                        </tr>

                        <tr>
                            <td class="orga">12:00&ndash;13:30</td>
                            <td class="orga">Lunch</td>
                        </tr>

                        <tr>
                            <td class="orga" width="120px">13:30&ndash;14:15</td>
                            <td class="orga uline">Keynote 2<br></td>
                        </tr>
                        <tr>
                            <td></td>
                            <td>
                                <em class="paper">Emerging challenges in securing frontier AI systems</em>
                                <br><b>Mikel Rodriguez</b>, AI Red Teaming @ Google Deepmind
                            </td>
                        </tr>


                        <tr>
                            <td class="orga">14:15&ndash;14:45</td>
                            <td class="orga">Break</td>
                        </tr>

                        <tr>
                            <td class="orga" width="120px">14:45&ndash;15:30</td>
                            <td class="orga uline">Keynote 3<br></td>
                        </tr>
                        <tr>
                            <td></td>
                            <td>
                                <em class="paper">Trustworthy AI and A Cybersecurity Perspective on Large Language Models</em>
                                <br><b>Mario Fritz</b>, Faculty @ CISPA Helmholtz Center for Information Security
                            </td>
                        </tr>
                        </tr>
                        
                        <tr>
                            <td class="orga">15:30&ndash;16:30</td>
                            <td class="orga uline">Poster session 2</td>
                        </tr>
                        

                        <tr>
                            <td class="orga">16:30&ndash;16:45</td>
                            <td class="orga">Closing remarks</td>
                        </tr>


                    </table>
                </div>
            </div>
        </div>

    <section id="accepted"><!-- Accepted papers -->
    <div class="container">
        <div class="row">
            <div class="col-lg-12 text-justify">
                <h2 class="section-heading text-uppercase">Accepted Papers</h2>
                
                <p>You can find the accepted papers in the <a href="https://dl.acm.org/doi/proceedings/10.1145/3605764">proceedings</a>.</p>

                <table cellpadding="5">
                    
                    <tr>
                        <td></td>
                        <td><em class="paper">Equivariant Differentially Private Deep Learning: Why DP-SGD Needs Sparser Models</em>
                            <br><b>Authors</b>: Florian A. Hölzl (Artifical Intelligence in Medicine, Technical University of Munich), Daniel Rueckert (Artifical Intelligence in Medicine, Technical University of Munich), Georgios Kaissis (Artifical Intelligence in Medicine, Technical University of Munich)
                        </td>
                    </tr>

                    <tr>
                        <td></td>
                        <td><em class="paper">When Side-Channel Attacks Break the Black-Box Property of Embedded Artificial Intelligence</em>
                            <br><b>Authors</b>: Benoit Coqueret (Univ. Rennes, Inria), Mathieu Carbone (Thales ITSEF), Olivier Sentieys (Univ. Rennes, Inria), Gabriel Zaid (Thales ITSEF)
                        </td>
                    </tr>

                    <tr>
                        <td></td>
                        <td><em class="paper">Probing the Transition to Dataset-Level Privacy in ML Models Using an Output-Specific and Data-Resolved Privacy Profile</em>
                            <br><b>Authors</b>: Tyler LeBlond (Booz Allen Hamilton), Joseph Munoz (Booz Allen Hamilton), Fred Lu (Booz Allen Hamilton), Maya Fuchs (Booz Allen Hamilton), Elliot Zaresky-Williams (Booz Allen Hamilton), Edward Raff (Booz Allen Hamilton), Brian Testa (Air Force Research Laboratory)
                        </td>
                    </tr>

                    <tr>
                        <td></td>
                        <td><em class="paper">Information Leakage from Data Updates in Machine Learning Models</em>
                            <br><b>Authors</b>: Tian Hui (The University of Melbourne), Farhad Farokhi (University of Melbourne), Olga Ohrimenko (The University of Melbourne)
                        </td>
                    </tr>

                    <tr>
                        <td></td>
                        <td><em class="paper">Membership Inference Attacks Against Semantic Segmentation Models</em>
                            <br><b>Authors</b>: Tomas Chobola (Helmholtz AI), Dmitrii Usynin (Department of Computing, Imperial College London; Artificial Intelligence in Medicine and Healthcare, TUM), Georgios Kaissis (Artificial Intelligence in Medicine and Healthcare, TUM; Institute for Machine Learning in Biomedical Imaging, Helmholtz Zentrum München; Department of Computing, Imperial College London)
                        </td>
                    </tr>

                    <tr>
                        <td></td>
                        <td><em class="paper">AVScan2Vec: Feature Learning on Antivirus Scan Data for Production-Scale Malware Corpora</em>
                            <br><b>Authors</b>: Robert J. Joyce (Booz Allen Hamilton, University of Maryland Baltimore County), Tirth Patel (University of Maryland Baltimore County), Charles Nicholas (University of Maryland Baltimore County), Edward Raff (Booz Allen Hamilton, University of Maryland Baltimore County)
                        </td>
                    </tr>

                    <tr>
                        <td></td>
                        <td><em class="paper">Utility-preserving Federated Learning</em>
                            <br><b>Authors</b>: Reza Nasirigerdeh (Technical University of Munich), Daniel Rueckert (Technical University of Munich), Georgios Kaissis (Technical University of Munich)
                        </td>
                    </tr>

                    <tr>
                        <td></td>
                        <td><em class="paper">Dictionary Attack on IMU-based Gait Authentication</em>
                            <br><b>Authors</b>: Rajesh Kumar (Bucknell University), Can Isik (Syracuse University), CHILUKURI MOHAN (Syracuse University)
                        </td>
                    </tr>

                    <tr>
                        <td></td>
                        <td><em class="paper">Differentially Private Logistic Regression with Sparse Solutions</em>
                            <br><b>Authors</b>: Amol Khanna (Booz Allen Hamilton), Fred Lu (Booz Allen Hamilton; University of Maryland, Baltimore County), Edward Raff (Booz Allen Hamilton; University of Maryland, Baltimore County), Brian Testa (Air Force Research Laboratory)
                        </td>
                    </tr>

                    <tr>
                        <td></td>
                        <td><em class="paper">Measuring Equality in Machine Learning Security Defenses: A Case Study in Speech Recognition</em>
                            <br><b>Authors</b>: Luke E. Richards (University of Maryland, Baltimore County), Edward Raff (University of Maryland, Baltimore County; Booz Allen Hamilton), Cynthia Matuszek (University of Maryland, Baltimore County)
                        </td>
                    </tr>

                    <tr>
                        <td></td>
                        <td><em class="paper">The Adversarial Implications of Variable-Time Inference</em>
                            <br><b>Authors</b>: Dudi Biton (Ben Gurion University of the Negev), Aditi Misra (University of Toronto), Efrat Levy (Ben Gurion University of the Negev), Jaidip Kotak (Ben Gurion University of the Negev), Ron Bitton (Ben Gurion University of the Negev), Roei Schuster (Wild Moose), Nicolas Papernot (University of Toronto and Vector Institute), Yuval Elovici (Ben Gurion University of the Negev), Ben Nassi (Cornell Tech)
                        </td>
                    </tr>

                    <tr>
                        <td></td>
                        <td><em class="paper">Task-Agnostic Safety for Reinforcement Learning</em>
                            <br><b>Authors</b>: Md Asifur Rahman (Wake Forest University), Sarra Alqahtani (Wake Forest University)
                        </td>
                    </tr>

                    <tr>
                        <td></td>
                        <td><em class="paper">Certified Robustness of Static Deep Learning-based Malware Detectors against Patch and Append Attacks</em>
                            <br><b>Authors</b>: Daniel Gibert (CeADAR, University College Dublin), Giulio Zizzo (IBM Research Europe), Quan Le (CeADAR, University College Dublin)
                        </td>
                    </tr>

                    <tr>
                        <td></td>
                        <td><em class="paper">Broken Promises: Measuring Confounding Effects in Learning-based Vulnerability Discovery</em>
                            <br><b>Authors</b>: Erik Imgrund (SAP Security Research), Tom Ganz (SAP Security Research), Martin Härterich (SAP Security Research), Niklas Risse (Max-Planck-Institute for Security and Privacy), Lukas Pirch (Technische Universität Berlin), Konrad Rieck (Technische Universität Berlin)
                        </td>
                    </tr>

                    <tr>
                        <td></td>
                        <td><em class="paper">Reward Shaping for Happier Autonomous Cyber Security Agents</em>
                            <br><b>Authors</b>: Elizabeth Bates (The Alan Turing Institute), Vasilios Mavroudis (The Alan Turing Institute), Chris Hicks (The Alan Turing Institute)
                        </td>
                    </tr>

                    <tr>
                        <td></td>
                        <td><em class="paper">Certifiers Make Neural Networks Vulnerable to Availability Attacks</em>
                            <br><b>Authors</b>: Tobias Lorenz (CISPA Helmholtz Center for Information Security), Marta Kwiatkowska (University of Oxford), Mario Fritz (CISPA Helmholtz Center for Information Security)
                        </td>
                    </tr>

                    <tr>
                        <td></td>
                        <td><em class="paper">Drift Forensics of Malware Classifiers</em>
                            <br><b>Authors</b>: Theo Chow (King's College London), Zeliang Kan (King's College London), Lorenz Linhardt (Technische Universität Berlin), Lorenzo Cavallaro (University College London), Daniel Arp (Technische Universität Berlin), Fabio Pierazzi (King's College London)
                        </td>
                    </tr>

                    <tr>
                        <td></td>
                        <td><em class="paper">Lookin' Out My Backdoor! Investigating Backdooring Attacks Against DL-driven Malware Detectors</em>
                            <br><b>Authors</b>: Mario D'Onghia (Politecnico di Milano), Federico Di Cesare (Politecnico di Milano), Luigi Gallo (Cyber Security Lab, Telecom Italia), Michele Carminati (Politecnico di Milano), Mario Polino (Politecnico di Milano), Stefano Zanero (Politecnico di Milano)
                        </td>
                    </tr>

                    <tr>
                        <td></td>
                        <td><em class="paper">Raze to the Ground: Query-Efficient Adversarial HTML Attacks on Machine-Learning Phishing Webpage Detectors</em>
                            <br><b>Authors</b>: Biagio Montaruli (SAP Security Research, EURECOM), Luca Demetrio (Università degli Studi di Genova), Maura Pintor (University of Cagliari), Battista Biggio (University of Cagliari), Luca Compagna (SAP Security Research), Davide Balzarotti (EURECOM)
                        </td>
                    </tr>

                    <tr>
                        <td></td>
                        <td><em class="paper">Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection</em>
                            <br><b>Authors</b>: Sahar Abdelnabi (CISPA Helmholtz Center for Information Security), Kai Greshake (Saarland University, sequire technology GmbH), Shailesh Mishra (Saarland University), Christoph Endres (sequire technology GmbH), Thorsten Holz (CISPA Helmholtz Center for Information Security), Mario Fritz (CISPA Helmholtz Center for Information Security)
                        </td>
                    </tr>

                    <tr>
                        <td></td>
                        <td><em class="paper">Canaries and Whistles: Resilient Drone Communication Networks with (or without) Deep Reinforcement Learning</em>
                            <br><b>Authors</b>: Chris Hicks (The Alan Turing Institute), Vasilios Mavroudis (The Alan Turing Institute), Myles Foley (Imperial College London), Thomas Davies (The Alan Turing Institute), Kate Highnam (Imperial College London), Tim Watson (The Alan Turing Institute)
                        </td>
                    </tr>

                </table>
            </div>
        </div>

    </section>

    <!-- </section>
    <section id="cfp">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-justify">
                    <h2 class="section-heading text-uppercase">Call for Papers</h2>
                    <h3 class="section-subheading">Important Dates</h3>
                    <ul>
                        <li>Paper submission deadline: <s>June 24</s> July 14th, 2023, 11:59 PM (all deadlines are AoE, UTC-12)</li>
                        <li>Reviews due: <s>July 25</s> August 8th, 2023</li>
                        <li>Review Released and Acceptance notification: <s>August 5th</s> August 13th, 2023</li>
                        <li>Camera ready due: September 5, 2023</li>
                        <li>Workshop: November 30, 2023</li>
                    </ul>
                    <h3 class="section-subheading">Overview</h3>
                    <p>Recent years have seen a dramatic increase in applications of artificial intelligence, machine
                        learning, and data mining to security and privacy problems. The use of AI and ML in
                        security-sensitive domains, in which adversaries may attempt to mislead or evade intelligent
                        machines, creates new frontiers for security research. The recent widespread adoption of deep
                        learning techniques, whose security properties are difficult to reason about directly, has only
                        added to the importance of this research. The AISec workshop, now in its 15th year, is the
                        leading venue for presenting and discussing new developments in the intersection of security and
                        privacy with AI and machine learning.</p>
                    <h3 class="section-subheading">Topics of Interest</h3>
                    <p>Topics of interest include (but are not limited to):</p>

                    <p>Theoretical topics related to security</p>
                    <ul>
                        <li>Adversarial learning</li>
                        <li>Security of deep learning systems</li>
                        <li>Robust statistics</li>
                        <li>Learning in games</li>
                        <li>Economics of security</li>
                        <li>Differential privacy</li>
                    </ul>

                    <p>Security applications</p>
                    <ul>
                        <li>Computer forensics</l1>
                        <li>Spam detection</li>
                        <li>Phishing detection and prevention</li>
                        <li>Botnet detection</li>
                        <li>Intrusion detection and response</li>
                        <li>Malware identification and analysis</li>
                        <li>Data anonymization/de-anonymization</li>
                        <li>Security in social networks</li>
                        <li>Big data analytics for security</li>
                        <li>User authentication</li>
                    </ul>

                    <p>Security-related AI problems
                    <p>
                    <ul>
                        <li>Distributed inference and decision making for security</li>
                        <li>Secure multiparty computation and cryptographic approaches</li>
                        <li>Privacy-preserving data mining</li>
                        <li>Adaptive side-channel attacks</li>
                        <li>Design and analysis of CAPTCHAs</li>
                        <li>AI approaches to trust and reputation</li>
                        <li>Vulnerability testing through intelligent probing (e.g. fuzzing)</li>
                        <li>Content-driven security policy management & access control</li>
                        <li>Techniques and methods for generating training and test sets</li>
                        <li>Anomalous behavior detection (e.g. for the purpose of fraud detection)</li>
                        <li>Model confidentiality</li>
                    </ul>



                    <h3 class="section-subheading">Submission Guidelines</h3>
                    <p>We invite the following types of papers:
                    <ul>
                        <li><b>Original research papers</b> on any topic in the intersection of AI or machine learning
                            with security, privacy, or related areas.</li>
                        <li><b>Position and open-problem papers</b> discussing the relationship of AI or machine
                            learning to security or privacy. Submitted papers of this type may not substantially overlap
                            with papers that have been published previously or that are simultaneously submitted to a
                            journal or conference/workshop proceedings.</li>
                        <li><b>Systematization-of-knowledge papers</b>, which should distill the AI or machine learning
                            contributions of a previously-published series of security papers.</li>
                    </ul>
                    </p>
                    <p>The authors can specify the paper type in the submission form. Paper submissions must be at most
                        10 pages in double-column ACM format, excluding the bibliography and well-marked appendices, and
                        at most 12 pages overall.
                        Papers should be in LaTeX and we recommend using the ACM format. This format is required for the
                        camera-ready version. Please follow the main CCS formatting instructions (except with page
                        limits as described above). In particular, we recommend using the sigconf template, which can be
                        downloaded from <a href="https://www.acm.org/publications/proceedings-template"
                            target="_blank">https://www.acm.org/publications/proceedings-template</a>. Accepted papers
                        will be published by the ACM Digital Library and/or ACM Press. Committee members are not
                        required to read the appendices, so the paper should be intelligible without them.
                        <u>Submissions must be in English and properly anonymized.</u>
                    </p>
                    <h3 class="section-subheading">Submission Site</h3>
                    <p>Submission link: <a href="https://aisec2023.hotcrp.com"
                            target="_blank">https://aisec2023.hotcrp.com</a>.</p>
                    <p>All accepted submissions will be presented at the workshop as posters. Accepted papers 
                        will be selected for presentation as spotlights based on their review score and novelty. 
                        Nonetheless, all accepted papers should be considered as having equal importance and 
                        will be included in the ACM workshop proceedings.</p>
                    <p>One author of each accepted paper is required to attend the workshop and present the paper for
                        it to be included in the proceedings.
                    </p>

                    <p>For any questions, please contact one the workshop organizers at <a
                            href="mailto:maura.pintor@unica.it">maura.pintor@unica.it</a></p>
                </div>
            </div>
        </div>
    </section> -->


    </section><!-- award -->
    <section id="award">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-justify">
                    <h2 class="section-heading text-uppercase">Best Paper Award</h2>
                    <!-- <h3 class="section-subheading">Best Paper Award</h3> -->
                    <p>As in the previous editions of this workshop, we would honor outstanding contributions.
                        To this end, we will award the best paper. The best paper will be selected by the reviewers
                        among all the submitted papers.</p>

                    <p>In the previous edition, <b>Stuart Millar</b> (Rapid7 LLC), <b>Denis Podgurskii</b> (OWASP), <b>Dan Kuykendall</b> (Rapid7 LLC), <b>Jesus Martinez del Rincon</b>, <b>Paul Miller</b> (Centre for Secure Information Technologies, Queen's University Belfast) were awarded the 2022 AISec
                        Best Paper Award for their work on “<b>Optimising Vulnerability Triage in DAST with Deep Learning</b>”.
                    </p>
                </div>
            </div>
        </div>
    </section>

    <!-- Committee -->
    <section id="committee" class="bg-light">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-left">
                    <h2 class="section-heading text-uppercase">Committee</h2>
                    <h3 class="section-subheading">Workshop Chairs</h3>
                    <ul class="noindent">
                        <li><a href="https://maurapintor.github.io/" target="_blank">Maura Pintor</a>,
                            University of Cagliari, Italy</li>
                        <li><a href="https://jungyhuk.github.io/" target="_blank">Xinyun Chen</a>, Google Brain, USA
                        </li>
                        <li><a href="https://floriantramer.com/" target="_blank">Florian Tramèr</a>, ETH Zürich,
                            Switzerland</li>
                    </ul>
                    <h3 class="section-subheading">Steering Committee</h3>
                    <ul class="noindent">
                        <li>
                            <a href="http://theory.stanford.edu/~dfreeman/" target="_blank">David Freeman</a>, Facebook,
                            Inc.
                        </li>
                        <li>
                            <a href="https://cis.unimelb.edu.au/people/staff.php?person_ID=20074/"
                                target="_blank">Benjamin Rubinstein</a>, University of Melbourne
                        </li>
                        <li><a href="https://pralab.diee.unica.it/en/BattistaBiggio/" target="_blank">Battista
                                Biggio</a>, University of Cagliari & PluribusOne</li>
                    </ul>
                    <h3 class="section-subheading">Program Committee</h3>

                    <ul class="noindent">
                        <li>Alessandro Brighente (University of Padova)</li>
                        <li>Ambra Demontis (University of Cagliari)</li>
                        <li>Andy Applebaum (Apple)</li>
                        <li>Angelo Sotgiu (CINI Consortium / University of Cagliari)</li>
                        <li>Ankit Gangwal (IIIT Hyderabad)</li>
                        <li>Antonio Emanuele Cinà (University of Genoa)</li>
                        <li>Arjun Nitin Bhagoji (University of Chicago)</li>
                        <li>Azqa Nadeem (TU Delft)</li>
                        <li>Battista Biggio (University of Cagliari)</li>
                        <li>Benjamin M. Ampel (University of Arizona)</li>
                        <li>Bobby Filar (Sublime Security)</li>
                        <li>Boyang Zhang (CISPA Helmholtz Center for Information Security)</li>
                        <li>Brad Miller (Twitter)</li>
                        <li>Chawin Sitawarin (UC Berkeley)</li>
                        <li>Christian Wressnegger (Karlsruhe Institute of Technology (KIT))</li>
                        <li>Clarence Chio (UC Berkeley)</li>
                        <li>Clinton Cao (Delft University of Technology)</li>
                        <li>Daniele Angioni (Università degli Studi di Cagliari)</li>
                        <li>Daniël Vos (Delft University of Technology )</li>
                        <li>Davide Maiorca (University of Cagliari, Italy)</li>
                        <li>Dmitrijs Trizna (University of Genova, Microsoft, Sapienza University of Rome)</li>
                        <li>Dongdong She (Columbia University/HKUST)</li>
                        <li>Edoardo Debenedetti (ETH Zurich)</li>
                        <li>Erwin Quiring (ICSI Berkeley, Ruhr University Bochum)</li>
                        <li>Fabio De Gaspari (Sapienza University of Rome)</li>
                        <li>Giacomo Quadrio (University of Padova)</li>
                        <li>Giorgio Piras (University of Cagliari)</li>
                        <li>Giorgio Severi (Northeastern University)</li>
                        <li>Giovanni Apruzzese (University of Liechtenstein)</li>
                        <li>Giulio Rigoni (University of Padua)</li>
                        <li>Hari Venugopalan (UC Davis)</li>
                        <li>Ilia Shumailov (University of Oxford)</li>
                        <li>Javier Carnerero Cano (Imperial College London)</li>
                        <li>Kathrin Grosse (EPFL)</li>
                        <li>Kexin Pei (Columbia University)</li>
                        <li>Lorenzo Cavallaro (University College London)</li>
                        <li>Luca Demetrio (Università degli Studi di Genova)</li>
                        <li>Luis Muñoz-González (Imperial College London)</li>
                        <li>Maria Rigaki (Czech Technical University)</li>
                        <li>Matthew Jagielski (Google)</li>
                        <li>Mauro Conti (University of Padua, TU Delft)</li>
                        <!-- <li>Melody Wolk (Apple)</li> -->
                        <!-- <li>Mingshen Sun (TikTok)</li> -->
                        <li>Pratyusa Manadhata (Meta)</li>
                        <li>Raouf Kerkouche (CISPA Helmholtz Center for Information Security)</li>
                        <li>Sagar Samtani (Indiana University)</li>
                        <li>Sahar Abdelnabi (CISPA Helmholtz Center for Information Security)</li>
                        <li>Sam Bretheim (Craigslist)</li>
                        <li>Sanghyun Hong (Oregon State University)</li>
                        <li>Scott Coull (Google)</li>
                        <li>Shiqi Wang (Amazon)</li>
                        <li>Shrikant Tangade (University of Padova, CHRIST University)</li>
                        <!-- <li>Stefano Traverso (Ermes Cyber Security SpA)</li> -->
                        <li>Thijs van Ede (University of Twente)</li>
                        <li>Tobias Lorenz (CISPA Helmholtz Center for Information Security)</li>
                        <li>Tom Ganz (SAP SE)</li>
                        <li>Vera Rimmer (DistriNet, KU Leuven)</li>
                        <li>Vikash Sehwag (Princeton University)</li>
                        <li>Vinod Puthuvath (Marie Curie Fellow, Cochin University)</li>
                        <li>Yang Zhang (CISPA Helmholtz Center for Information Security)</li>
                        <li>Yash Vekaria (University of California, Davis)</li>
                        <li>Zied Ben Houidi (Huawei Technologies Co. Ltd.)</li>
                        <li>Ziqi Yang (Zhejiang University)</li>
                    </ul>
                      </div>
            </div>
        </div>

    </section><!-- Footer -->
    <footer>
        <div class="container">
            <div class="row">
                <div class="col-md-6">
                    <span class="copyright">Copyright © AISec 2023</span><br>
                </div>
                <div class="col-md-6">
                    Support kindly provided by the <a href="https://www.unica.it/unica/en/homepage.page/"
                        target="_blank">University of Cagliari</a> and by the <a href="https://elsa-ai.eu"
                        target="_blank">ELSA project</a>.
                </div>
            </div>
        </div>
    </footer>
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha3/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-ENjdO4Dr2bkBIFxQpeoTz1HIcje39Wm4jDKdf19U8gI4ddQ3GYNS7NTKfAdVQSZe"
        crossorigin="anonymous"></script>

    <script src="js/agency.min.js"></script>
</body>

</html>